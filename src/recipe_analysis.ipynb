{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricette definite in un formato semistrutturato, raggruppamento degli step e ingredienti in un unica stringa, funzioni di utilità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni util e import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.sem import relextract\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.corpus import conll2000\n",
    "from spacy.symbols import X, NUM, VERB, NOUN, ADP\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('conll2000')\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "NLP = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "def string_recipe(i):\n",
    "    return dataset.iloc[i]['title'] + \"\\n\\n\" + dataset.iloc[i]['ingredients'] + \"\\n\\n\" + dataset.iloc[i]['step'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = \".\\n\".join(literal_eval(dataset.iloc[index]['ingredients']))\n",
    "    dataset.iloc[index]['step'] = \" \".join(literal_eval(dataset.iloc[index]['step']))\n",
    "\n",
    "display(dataset.head())\n",
    "print(string_recipe(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione delle abbreviazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase iniziale di ritrovamento del set di abbreviazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrv_dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "abbrv = set()\n",
    "for index in range(len(dataset)):\n",
    "    abbrv_dataset.iloc[index]['ingredients'] = \" \".join(literal_eval(abbrv_dataset.iloc[index]['ingredients']))\n",
    "    for element in re.findall(r\"[A-Za-z]*\\.\", abbrv_dataset.iloc[index]['ingredients']):\n",
    "        abbrv.add(element)\n",
    "    \n",
    "print(abbrv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimozione delle abbreviazioni in quanto possono essere dannose per il processo di tokenizzazione. Es pkg. ---> package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviations(ingredients_string):\n",
    "    __ABBREVIATIONS__ = {\n",
    "        'pkg.'  :   'package',\n",
    "        'tsb.'  :   'tablespoon',\n",
    "        'no.'   :   'number',\n",
    "        'pt.'   :   'pint',\n",
    "        'no.'   :   'number',\n",
    "        'gal.'  :   'gallon',\n",
    "        'tbsp.' :   'tablespoon',\n",
    "        'sq.'   :   'square',\n",
    "        'oz.'   :   'ounce',\n",
    "        'lb.'   :   'pound',\n",
    "        'qt.'   :   'quart',\n",
    "        'c.'    :   'cup',\n",
    "        'tsp.'  :   'teaspoon'\n",
    "    }\n",
    "    for item, value in __ABBREVIATIONS__.items():\n",
    "        ingredients_string = ingredients_string.lower().replace(item, value)\n",
    "    return ingredients_string\n",
    "\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = expand_abbreviations(dataset.iloc[index]['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting\n",
    "Il contenuto delle colonne 'ingredients' e 'step' verrà suddiviso in frasi. In precedenza i periodi contenuti nelle singole celle sono stati formattati in modo tale da renderli riconoscibili e facilmente suddivisibili in frasi ben separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['ingredients'] = (sent_tokenize(dataset.iloc[index]['ingredients']))\n",
    "print(dataset.iloc[10]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['step'] = (sent_tokenize(dataset.iloc[index]['step']))\n",
    "print(dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimozione quantità doppie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_reg = r'\\d*\\s*\\(.*\\)'\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    for value in range(len(dataset.iloc[index]['ingredients'])):\n",
    "        elements = re.findall(dr_reg, dataset.iloc[index]['ingredients'][value])\n",
    "        for e in elements:\n",
    "            new_string = dataset.iloc[index]['ingredients'][value].replace(e,e[e.find(\"(\")+1: e.find(\")\")].strip())\n",
    "            dataset.iloc[index]['ingredients'][value] = new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Nella colonna 'step' troviamo una serie di passaggi da compiere per creare la ricetta. Questi passaggi sono scritti in linguaggio naturale e possono essere semplificati rimuovendo delle parole dette stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in swr_dataset\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "swr_dataset = dataset.copy(deep=True)\n",
    "\n",
    "tk = lambda x,st: ' '.join([w for w in x if w not in st])\n",
    "for index in range(len(dataset)):\n",
    "    swr_dataset.iloc[index]['step'] = [tk(word_tokenize(sent), stop_words) for sent in dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming e Lemming\n",
    "Questi due processi potrebbero portare valore all'analisi del dominio. Il codice per entrambi viene proposto qui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in lem_dataset\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_dataset = swr_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(swr_dataset)):\n",
    "    lem_dataset.iloc[index]['step'] = stem_sent = [' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(sent)]) for sent in swr_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in stm_dataset\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stm_dataset = lem_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(lem_dataset)):\n",
    "    stm_dataset.iloc[index]['step'] = [' '.join([stemmer.stem(w) for w in word_tokenize(sent)]) for sent in lem_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampa dei risultati delle operazioni di stop word removal, stemming e lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frasi originali\\n\")\n",
    "print(dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word removal\\n\")\n",
    "print(swr_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word e lemming\\n\")\n",
    "print(lem_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word lemming e stemming\\n\")\n",
    "print(stm_dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entityt Extraction, Relation Extraction e POS tagging con SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dataset = dataset.copy(deep=True)\n",
    "       \n",
    "for index in range(len(dataset)):\n",
    "    spacy_dataset.iloc[index]['ingredients'] = [NLP(element) for element in dataset.iloc[index]['ingredients']]\n",
    "    spacy_dataset.iloc[index]['step'] = [NLP(element) for element in dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pprintsp__(dataset, index, column, value, extend=False):\n",
    "    displacy.render(dataset.iloc[index][column][value], style='dep')\n",
    "    print(dataset.iloc[index][column][value])\n",
    "    displacy.render(dataset.iloc[index][column][value], style='ent')\n",
    "    if extend:\n",
    "        for token in dataset.iloc[index][column][value]:\n",
    "            print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "def pprint_spacyd_all(dataset, index, column, extend=False):\n",
    "    for v in range(len(dataset.iloc[index][column])):\n",
    "        __pprintsp__(dataset, index, column, v, extend)\n",
    "            \n",
    "def pprint_spacyd(dataset, index, column, value, extend=False):\n",
    "    __pprintsp__(dataset, index, column, value, extend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi degli ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe:\n",
    "    def __init__(self, title, index, ingredients_str, steps_str):\n",
    "        self.title = title\n",
    "        self.ingredients = []\n",
    "        self.ing_str = ingredients_str\n",
    "        self.stp_str = steps_str\n",
    "        self.steps = []\n",
    "        self.idx = index\n",
    "    \n",
    "    def add_ing(self, ing):\n",
    "        self.ingredients.append(ing)\n",
    "        \n",
    "    def __str__(self):\n",
    "        _ing_s = '\\n'.join([ing.short_srt() for ing in self.ingredients])\n",
    "        _stp_s = '\\n'.join(str(step) for step in self.steps)\n",
    "        _header_ing = f\"|{'NOME':<40s}|{'PROPRIETÀ':<20s}|{'QUANTITÀ':<11s}|{'TIPO':<21s}|\"\n",
    "        _header_ing_l = f\"+{'-'*40}+{'-'*20}+{'-'*11}+{'-'*21}+\"\n",
    "        _header_step = f\"|{'N°':<2s}|{'AZIONE':<15s}|{'STRUMENTI':<20s}|{'INGREDIENTI':<55s}|\"\n",
    "        _header_step_l = f\"+{'-'*2}+{'-'*15}+{'-'*20}+{'-'*55}|\"\n",
    "        return f\"TITOLO: {self.title}\\n\\n{_header_ing}\\n{_header_ing_l}\\n{_ing_s}\\n{_header_ing_l}\\n\\n{_header_step}\\n{_header_step_l}\\n{_stp_s}\\n{_header_step_l}\"\n",
    "    \n",
    "    def print_unstructured(self):\n",
    "        return f\"{self.title}\\n\\n{self.ing_str}\\n\\n{self.stp_str}\"\n",
    "    \n",
    "    \n",
    "class Step:\n",
    "    def __init__(self, step_str, step_no, action, ins, ing):\n",
    "        self.action = action\n",
    "        self.ins = ins\n",
    "        self.ing = ing\n",
    "        self.step_str = step_str\n",
    "        self.step_no = step_no\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"|{self.step_no:<2d}|{self.action:<15s}|{','.join(self.ins):20s}|{','.join(self.ing):55s}|\"\n",
    "\n",
    "    \n",
    "    \n",
    "class Ingredient:\n",
    "    def __init__(self, full_text):\n",
    "        self.name=\"\"\n",
    "        self.ing_cat_id = None\n",
    "        self.size=\"\"\n",
    "        self.quantity= 0.0\n",
    "        self.size=\"\"\n",
    "        self.adj=[]\n",
    "        self.original=full_text\n",
    "    def __str__(self):\n",
    "        return f\"Name: {self.name}\\nAjdectives: {self.adj}\\nQuantity: {self.quantity:4f} {self.size}\\nOriginal: {self.original}\"\n",
    "\n",
    "    def short_srt(self):\n",
    "        return f\"|{self.name:<40s}|{','.join(self.adj):<20s}|{self.quantity:^11.2f}|{self.size:<21s}|\"\n",
    "    \n",
    "    def set_id(self, identifier):\n",
    "        self.ing_cat_id = identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione elenco ingredienti e ricette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_food(word):\n",
    "    syns = wn.synsets(str(word), pos = wn.NOUN)\n",
    "    for syn in syns:\n",
    "        if 'food' in syn.lexname():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def cls_w(word):\n",
    "    return word.strip().lower()\n",
    "\n",
    "def parse_quantity(qt):\n",
    "    if qt == \"\":\n",
    "        return 1\n",
    "    \n",
    "    partial_sum = 0.0\n",
    "    backtrack_multiplier = 1.0\n",
    "    \n",
    "    for elem in qt.split(\" \"):\n",
    "        try:\n",
    "        \n",
    "            if ('/' in elem) and (partial_sum == 0.0):\n",
    "                parsed = elem.split(\"/\")\n",
    "                backtrack_multiplier = float(parsed[0])/float(parsed[1])\n",
    "\n",
    "            elif ('/' in elem) and (partial_sum != 0.0):\n",
    "                parsed = elem.split(\"/\")\n",
    "                partial_sum += (float(parsed[0])/float(parsed[1]))\n",
    "\n",
    "            else:\n",
    "                partial_sum += float(elem)\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "    if(partial_sum != 0.0):\n",
    "        partial_sum *= backtrack_multiplier\n",
    "    else:\n",
    "        partial_sum = backtrack_multiplier\n",
    "        \n",
    "    return partial_sum\n",
    "            \n",
    "        \n",
    "\n",
    "#Root generation\n",
    "\n",
    "__SIZES__ = [\n",
    "       'package',\n",
    "       'tablespoon',\n",
    "       'number',\n",
    "       'pint',\n",
    "       'number',\n",
    "       'gallon',\n",
    "       'tablespoon',\n",
    "       'square',\n",
    "       'ounce',\n",
    "       'pound',\n",
    "       'quart',\n",
    "       'cup',\n",
    "       'teaspoon',\n",
    "       'can'\n",
    "]\n",
    "\n",
    "\n",
    "all_recipes = []\n",
    "    \n",
    "for index in range(len(spacy_dataset)):\n",
    "    \n",
    "    recipe = Recipe(\n",
    "        spacy_dataset.iloc[index]['title'], \n",
    "        index, \n",
    "        \" \".join(dataset.iloc[index]['ingredients']),\n",
    "        \" \".join(dataset.iloc[index]['step'])       \n",
    "    )\n",
    "    \n",
    "    for element in spacy_dataset.iloc[index]['ingredients']:\n",
    "\n",
    "        ingredient = element\n",
    "\n",
    "        IGN = Ingredient(ingredient.text)\n",
    "        ROOT_NODE = [token for token in ingredient if token.dep_ == 'ROOT'][0]\n",
    "        \n",
    "\n",
    "        if (ROOT_NODE.pos == VERB):\n",
    "                IGN.adj.append(cls_w(ROOT_NODE.text))\n",
    "        else:\n",
    "                IGN.name = cls_w(ROOT_NODE.text)\n",
    "                \n",
    "        stack = [element for element in ROOT_NODE.children]\n",
    "\n",
    "        unparsed_quantity = \"\"\n",
    "        \n",
    "        while len(stack)!=0:\n",
    "            CURRENT_NODE = stack.pop()\n",
    "\n",
    "            if (CURRENT_NODE.pos == X) or (CURRENT_NODE.pos == NUM):\n",
    "                    unparsed_quantity = CURRENT_NODE.text + \" \" + unparsed_quantity\n",
    "            else:\n",
    "                if CURRENT_NODE.text.lower() in  __SIZES__:\n",
    "                    IGN.size += \" \" + cls_w(CURRENT_NODE.text)\n",
    "\n",
    "                if ((CURRENT_NODE.dep_ == 'compound') or (CURRENT_NODE.dep_ == 'dobj') or (CURRENT_NODE.dep_ == 'pobj')) and is_food(CURRENT_NODE.text) and (CURRENT_NODE.text.lower() not in  __SIZES__):\n",
    "                    IGN.name = CURRENT_NODE.text + \" \" + cls_w(IGN.name)\n",
    "\n",
    "                if (CURRENT_NODE.dep_ == 'amod') or (CURRENT_NODE.dep_ == 'appos'):\n",
    "                    if is_food(CURRENT_NODE.text):\n",
    "                        IGN.name = CURRENT_NODE.text + \" \" + cls_w(IGN.name)\n",
    "                    else:\n",
    "                        IGN.adj.append(cls_w(CURRENT_NODE.text))         \n",
    "\n",
    "            stack += [element for element in CURRENT_NODE.children]\n",
    "        \n",
    "        if IGN.name.strip() != \"\":\n",
    "            IGN.quantity = parse_quantity(cls_w(unparsed_quantity))\n",
    "            IGN.name = cls_w(IGN.name)\n",
    "            recipe.add_ing(IGN)\n",
    "    all_recipes.append(recipe)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi degli step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_all_words = set()\n",
    "\n",
    "for element in all_recipes:\n",
    "    for ingredient in element.ingredients:\n",
    "        for e in ingredient.name.split(\" \"):\n",
    "            ingredients_all_words.add(cls_w(e))\n",
    "\n",
    "ingredients_all_words.add('potato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_verb(word):\n",
    "    ss = wn.synsets(word)\n",
    "    for s in ss:\n",
    "        if s.pos() == 'v':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def combine_action(action_token):\n",
    "    action = cls_w(action_token.text)\n",
    "    for child in action_token.children:\n",
    "        if child.dep_ == 'prt' or child.dep_ == 'compound':\n",
    "            action += \" \" + cls_w(child.text)\n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_instrument(token):\n",
    "    stack = [element for element in token.children]\n",
    "    instrument = \"\"\n",
    "    found_main = False\n",
    "    main_node = None\n",
    "\n",
    "    while stack:\n",
    "        s_element = stack.pop()\n",
    "        if s_element.pos == NOUN and (cls_w(s_element.text) not in ingredients_all_words) and (not found_main) and (s_element.text not in forbidden_instruments):\n",
    "            if \"ed\" != s_element.text[-2:] and \"ing\" != s_element.text[-3:]:\n",
    "                instrument += cls_w(s_element.text)\n",
    "                found_main = True\n",
    "                main_node = s_element\n",
    "        elif s_element.dep_ == 'compound' and main_node == s_element.head and (cls_w(s_element.text) not in ingredients_all_words):\n",
    "            instrument = cls_w(s_element.text) + \" \" + instrument\n",
    "            main_node = s_element\n",
    "            \n",
    "        stack += [element for element in s_element.children]\n",
    "    return instrument\n",
    "\n",
    "\n",
    "forbidden_actions = ['done', 'will', 'be']\n",
    "forbidden_instruments = ['top']\n",
    "PREPS = ['with', 'in', 'on', 'of', 'using', 'into', 'use']\n",
    "\n",
    "def get_ingredients(recipe):\n",
    "    return {ing.name for ing in recipe.ingredients}\n",
    "\n",
    "def find_ingredients(ings, step):\n",
    "     \n",
    "    ing = set()\n",
    "    for word in step.replace(',','').replace('.','').replace('(','').replace(')','').replace('*','').split(\" \"):\n",
    "        word = cls_w(word)\n",
    "        for el in ings:\n",
    "            if re.findall(f\"(\\\\s{word}\\\\s)|({word}\\\\s)|(\\\\s{word})\", el) or el == word:\n",
    "                ing.add(el)                \n",
    "    return ing\n",
    "\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    step_counter = 1\n",
    "    ing_s = get_ingredients(all_recipes[index])\n",
    "    for value in range(len(dataset.iloc[index]['step'])):\n",
    "        for element in dataset.iloc[index]['step'][value].split(\";\"):\n",
    "               \n",
    "            step = NLP(element.replace(\"°\", '').strip())\n",
    "            \n",
    "            FINAL_ACTION = \"\"\n",
    "            _hidden_verbs = []\n",
    "            _found_action = False\n",
    "            found_instruments = []\n",
    "            found_ingredients = []\n",
    "            \n",
    "            for TOKEN in step:\n",
    "                if (\"ed\" != TOKEN.text[-2:]) and TOKEN.text not in forbidden_actions and (\"ing\" != TOKEN.text[-3:]):\n",
    "                    if not _found_action:\n",
    "                        if (TOKEN.pos == VERB):\n",
    "                            FINAL_ACTION = combine_action(TOKEN)\n",
    "                            _found_action = True\n",
    "                        elif hidden_verb(TOKEN.text):\n",
    "                            _hidden_verbs.append(combine_action(TOKEN))\n",
    "                            \n",
    "                if TOKEN.text in PREPS:\n",
    "                    instrument = find_instrument(TOKEN)\n",
    "                    if instrument != \"\":\n",
    "                        found_instruments.append(cls_w(instrument))\n",
    "                        \n",
    "                      \n",
    "            if FINAL_ACTION == \"\" and _hidden_verbs:\n",
    "                FINAL_ACTION = _hidden_verbs[0]\n",
    "                \n",
    "            if FINAL_ACTION != \"\":\n",
    "                found_step = Step(element, step_counter, cls_w(FINAL_ACTION), found_instruments, find_ingredients(ing_s, step.text))\n",
    "                all_recipes[index].steps.append(found_step)\n",
    "                step_counter += 1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump\n",
    "Tutte le ricette vengono stampate in formato testuale secondo suddivisione in ingredienti, strumenti e step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dump.txt', 'w') as f:\n",
    "    for i, index in enumerate(all_recipes):\n",
    "        f.write(index.print_unstructured() + '\\n')\n",
    "        f.write(f\"[{i}]\" + str(index) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAZIONE DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione indici per ricette e ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_ingredients = {}\n",
    "ingredients_idx = {}\n",
    "macro_actions = {}\n",
    "actions_idx = {}\n",
    "macro_instruments = {}\n",
    "instruments_idx = {}\n",
    "##\n",
    "actions_idx = {}\n",
    "\n",
    "for element in all_recipes:\n",
    "    for ingredient in element.ingredients:\n",
    "        if ingredient.name in macro_ingredients:\n",
    "            macro_ingredients[ingredient.name] += 1\n",
    "        else:\n",
    "            macro_ingredients[ingredient.name] = 1\n",
    "            \n",
    "for index, key in enumerate(macro_ingredients):\n",
    "    ingredients_idx[key] = index\n",
    "    #print(f\"{key:>50s} {macro_ingredients[key]:<3d}\")\n",
    "\n",
    "\n",
    "for element in all_recipes:\n",
    "    for step in element.steps:\n",
    "        if step.action in macro_actions:\n",
    "            macro_actions[step.action] += 1\n",
    "        else:\n",
    "            macro_actions[step.action] = 1\n",
    "            \n",
    "        for ins in step.ins:\n",
    "            if ins in macro_instruments:\n",
    "                macro_instruments[ins] += 1\n",
    "            else:\n",
    "                macro_instruments[ins] = 1\n",
    "            \n",
    "            \n",
    "#print(100*'-')\n",
    "\n",
    "for index, key in enumerate(macro_actions):\n",
    "    actions_idx[key] = index\n",
    "    #print(f\"{key:>50s} {macro_steps[key]:<3d}\")\n",
    "    \n",
    "#print(100*'-')\n",
    "\n",
    "for index, key in enumerate(macro_instruments):\n",
    "    instruments_idx[key] = index\n",
    "    #print(f\"{key:>50s} {macro_instruments[key]:<3d}\")\n",
    "    \n",
    "#print(instruments_idx)\n",
    "#print(steps_idx)\n",
    "#print(ingredients_idx)\n",
    "#print(actions_idx)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ING_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS ingredients;\n",
    "CREATE TABLE ingredients(\n",
    "id INT PRIMARY KEY,\n",
    "name VARCHAR(100) NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(ING_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_ingredients.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for ing in ingredients_idx:\n",
    "        ING_DB_CREATE = ING_DB_CREATE + f\"INSERT INTO ingredients VALUES ({ingredients_idx[ing]},\\\"{ing}\\\");\\n\"\n",
    "    ddl_file.write(ING_DB_CREATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL strumenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ING_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS instruments;\n",
    "CREATE TABLE instruments(\n",
    "id INT PRIMARY KEY,\n",
    "name VARCHAR(100) NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(ING_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_instruments.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for ins in instruments_idx:\n",
    "        ING_DB_CREATE = ING_DB_CREATE + f\"INSERT INTO instruments VALUES ({instruments_idx[ins]},\\\"{ins}\\\");\\n\"\n",
    "    ddl_file.write(ING_DB_CREATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL azioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ING_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS actions;\n",
    "CREATE TABLE actions(\n",
    "id INT PRIMARY KEY,\n",
    "name VARCHAR(100) NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(ING_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_actions.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for act in actions_idx:\n",
    "        ING_DB_CREATE = ING_DB_CREATE + f\"INSERT INTO actions VALUES ({actions_idx[act]},\\\"{act}\\\");\\n\"\n",
    "    ddl_file.write(ING_DB_CREATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL ricette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REC_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS recipes;\n",
    "CREATE TABLE recipes(\n",
    "id INT PRIMARY KEY,\n",
    "titolo VARCHAR(100) NOT NULL,\n",
    "preparazione_nstr VARCHAR(500),\n",
    "ingredienti_nstr VARCHAR(500)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(REC_DB_CREATE)\n",
    "\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_recipes.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for rec in all_recipes:\n",
    "        \n",
    "        corrected_ing = rec.ing_str.replace(\"\\'\",\"\\'\\'\")\n",
    "        corrected_stp = rec.stp_str.replace(\"\\'\",\"\\'\\'\")\n",
    "        corrected_tit = rec.title.replace(\"\\'\",\"\\'\\'\")\n",
    "        \n",
    "        REC_DB_CREATE = REC_DB_CREATE + f\"INSERT INTO recipes (id, titolo, ingredienti_nstr, preparazione_nstr) VALUES ({rec.idx},\\'{corrected_tit}\\',\\'{corrected_ing}\\',\\'{corrected_stp}\\');\\n\"\n",
    "    ddl_file.write(REC_DB_CREATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione DDL tabella relazione Ricette <--> Ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CON_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS contains;\n",
    "CREATE TABLE contains(\n",
    "id INT NOT NULL AUTO_INCREMENT,\n",
    "misura VARCHAR(50),\n",
    "quantita FLOAT,\n",
    "proprieta VARCHAR(100),\n",
    "recipeID int NOT NULL,\n",
    "ingredientID int NOT NULL,\n",
    "PRIMARY KEY (id),\n",
    "FOREIGN KEY (recipeID) REFERENCES recipes(id),\n",
    "FOREIGN KEY (ingredientID) REFERENCES ingredients(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(CON_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_contains.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for r in all_recipes:\n",
    "        for i in r.ingredients:\n",
    "            if i.name.strip() != \"\":\n",
    "                adj = ', '.join(i.adj)\n",
    "                data = f\"(\\'{i.size}\\',{i.quantity},\\'{adj}\\',{r.idx}, {ingredients_idx[i.name]})\"\n",
    "                CON_DB_CREATE = CON_DB_CREATE + f\"INSERT INTO contains (misura, quantita, proprieta, recipeID, ingredientID) VALUES {data};\\n\"\n",
    "    ddl_file.write(CON_DB_CREATE)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CON_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS steps;\n",
    "CREATE TABLE contains(\n",
    "id INT NOT NULL AUTO_INCREMENT,\n",
    "descrizione VARCHAR(500),\n",
    "numero_step INT,\n",
    "PRIMARY KEY (id),\n",
    "FOREIGN KEY (actionID) REFERENCES actions(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(CON_DB_CREATE)\n",
    "\n",
    "with open(Path(\"../database/create_steps.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for r in all_recipes:\n",
    "        for s in r.steps:\n",
    "            data = f\"(\\'{s.step_str}\\',{s.step_no}, {actions_idx[s.action]})\"\n",
    "            CON_DB_CREATE = CON_DB_CREATE + f\"INSERT INTO steps (descrizione, numero_step) VALUES {data};\\n\"\n",
    "    ddl_file.write(CON_DB_CREATE)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione DDL tabella relazione Step <--> Strumenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USE recipe_analysis;\n",
      "DROP TABLE IF EXISTS utilize;\n",
      "CREATE TABLE utilize(\n",
      "id INT NOT NULL AUTO_INCREMENT,\n",
      "PRIMARY KEY (id),\n",
      "FOREIGN KEY (instrumentID) REFERENCES instruments(id),\n",
      "FOREIGN KEY (stepID) REFERENCES steps(id)\n",
      ");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CON_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS utilize;\n",
    "CREATE TABLE utilize(\n",
    "id INT NOT NULL AUTO_INCREMENT,\n",
    "PRIMARY KEY (id),\n",
    "FOREIGN KEY (instrumentID) REFERENCES instruments(id),\n",
    "FOREIGN KEY (stepID) REFERENCES steps(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(CON_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_utilize.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for r in all_recipes:\n",
    "        for s in r.steps:\n",
    "            for i in s.ins:\n",
    "                data = f\"({instruments_idx[i]}, {s.step_no})\"\n",
    "                CON_DB_CREATE = CON_DB_CREATE + f\"INSERT INTO utilize (instrumentID, stepID) VALUES {data};\\n\"\n",
    "    ddl_file.write(CON_DB_CREATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione DDL tabella relazione Step <--> Ricette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USE recipe_analysis;\n",
      "DROP TABLE IF EXISTS composed;\n",
      "CREATE TABLE composed(\n",
      "id INT NOT NULL AUTO_INCREMENT,\n",
      "PRIMARY KEY (id),\n",
      "FOREIGN KEY (recipeID) REFERENCES recipes(id),\n",
      "FOREIGN KEY (stepID) REFERENCES steps(id)\n",
      ");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CON_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS composed;\n",
    "CREATE TABLE composed(\n",
    "id INT NOT NULL AUTO_INCREMENT,\n",
    "PRIMARY KEY (id),\n",
    "FOREIGN KEY (recipeID) REFERENCES recipes(id),\n",
    "FOREIGN KEY (stepID) REFERENCES steps(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(CON_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_composed.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for r in all_recipes:\n",
    "        for s in r.steps:\n",
    "            data = f\"({r.idx}, {s.step_no})\"\n",
    "            CON_DB_CREATE = CON_DB_CREATE + f\"INSERT INTO composed (recipeID, stepID) VALUES {data};\\n\"\n",
    "    ddl_file.write(CON_DB_CREATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione DDL tabella relazione Step <--> Ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USE recipe_analysis;\n",
      "DROP TABLE IF EXISTS involves;\n",
      "CREATE TABLE involves(\n",
      "id INT NOT NULL AUTO_INCREMENT,\n",
      "PRIMARY KEY (id),\n",
      "FOREIGN KEY (ingredientID) REFERENCES ingredients(id),\n",
      "FOREIGN KEY (stepID) REFERENCES steps(id)\n",
      ");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CON_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS involves;\n",
    "CREATE TABLE involves(\n",
    "id INT NOT NULL AUTO_INCREMENT,\n",
    "PRIMARY KEY (id),\n",
    "FOREIGN KEY (ingredientID) REFERENCES ingredients(id),\n",
    "FOREIGN KEY (stepID) REFERENCES steps(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(CON_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_involves.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for r in all_recipes:\n",
    "        for s in r.steps:\n",
    "            for i in s.ing:\n",
    "                data = f\"({ingredients_idx[i]}, {s.step_no})\"\n",
    "                CON_DB_CREATE = CON_DB_CREATE + f\"INSERT INTO involves (ingredientID, stepID) VALUES {data};\\n\"\n",
    "    ddl_file.write(CON_DB_CREATE)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fef19a476ade33c91668e3897e6fa668c17f7c4a539d9a83d441085d42ee0ef"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
