{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricette definite in un formato semistrutturato, raggruppamento degli step e ingredienti in un unica stringa, funzioni di utilità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni util e import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/navis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/navis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/navis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/navis/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/navis/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to /home/navis/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 45.7 MB 28.8 MB/s eta 0:00:01   |▉                               | 1.2 MB 3.1 MB/s eta 0:00:15     |██████████▍                     | 14.9 MB 3.1 MB/s eta 0:00:10     |████████████▎                   | 17.5 MB 9.9 MB/s eta 0:00:03     |████████████████████▌           | 29.3 MB 9.9 MB/s eta 0:00:02     |██████████████████████████▍     | 37.6 MB 28.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from en-core-web-md==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: setuptools in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/navis/anaconda3/envs/Progetto_GDNS/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.sem import relextract\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.corpus import conll2000\n",
    "from spacy.symbols import X, NUM, VERB, NOUN, ADP\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('conll2000')\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "NLP = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "def string_recipe(i):\n",
    "    return dataset.iloc[i]['title'] + \"\\n\\n\" + dataset.iloc[i]['ingredients'] + \"\\n\\n\" + dataset.iloc[i]['step'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>1 small jar chipped beef, cut up.\\n4 boned chi...</td>\n",
       "      <td>Place chipped beef on bottom of baking dish. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>2 (16 oz.) pkg. frozen corn.\\n1 (8 oz.) pkg. c...</td>\n",
       "      <td>In a slow cooker, combine all ingredients. Cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>1 large whole chicken.\\n2 (10 1/2 oz.) cans ch...</td>\n",
       "      <td>Boil and debone chicken. Put bite size pieces ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>1 c. peanut butter.\\n3/4 c. graham cracker cru...</td>\n",
       "      <td>Combine first four ingredients and press in 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cheeseburger Potato Soup</td>\n",
       "      <td>6 baking potatoes.\\n1 lb. of extra lean ground...</td>\n",
       "      <td>Wash potatoes; prick several times with a fork...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "index                             \n",
       "1         Jewell Ball'S Chicken   \n",
       "2                   Creamy Corn   \n",
       "3                 Chicken Funny   \n",
       "4          Reeses Cups(Candy)     \n",
       "5      Cheeseburger Potato Soup   \n",
       "\n",
       "                                             ingredients  \\\n",
       "index                                                      \n",
       "1      1 small jar chipped beef, cut up.\\n4 boned chi...   \n",
       "2      2 (16 oz.) pkg. frozen corn.\\n1 (8 oz.) pkg. c...   \n",
       "3      1 large whole chicken.\\n2 (10 1/2 oz.) cans ch...   \n",
       "4      1 c. peanut butter.\\n3/4 c. graham cracker cru...   \n",
       "5      6 baking potatoes.\\n1 lb. of extra lean ground...   \n",
       "\n",
       "                                                    step  \n",
       "index                                                     \n",
       "1      Place chipped beef on bottom of baking dish. P...  \n",
       "2      In a slow cooker, combine all ingredients. Cov...  \n",
       "3      Boil and debone chicken. Put bite size pieces ...  \n",
       "4      Combine first four ingredients and press in 13...  \n",
       "5      Wash potatoes; prick several times with a fork...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckeye Candy\n",
      "\n",
      "1 box powdered sugar.\n",
      "8 oz. soft butter.\n",
      "1 (8 oz.) peanut butter.\n",
      "paraffin.\n",
      "12 oz. chocolate chips\n",
      "\n",
      "Mix sugar, butter and peanut butter. Roll into balls and place on cookie sheet. Set in freezer for at least 30 minutes. Melt chocolate chips and paraffin in double boiler. Using a toothpick, dip balls 3/4 of way into chocolate chip and paraffin mixture to make them look like buckeyes.\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = \".\\n\".join(literal_eval(dataset.iloc[index]['ingredients']))\n",
    "    dataset.iloc[index]['step'] = \" \".join(literal_eval(dataset.iloc[index]['step']))\n",
    "\n",
    "display(dataset.head())\n",
    "print(string_recipe(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione delle abbreviazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase iniziale di ritrovamento del set di abbreviazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tsp.', 'gal.', 'pkg.', 'No.', 'Tbsp.', 'c.', 'oz.', 'lb.', 'sq.', 'pt.', 'tbsp.', 'qt.'}\n"
     ]
    }
   ],
   "source": [
    "abbrv_dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "abbrv = set()\n",
    "for index in range(len(dataset)):\n",
    "    abbrv_dataset.iloc[index]['ingredients'] = \" \".join(literal_eval(abbrv_dataset.iloc[index]['ingredients']))\n",
    "    for element in re.findall(r\"[A-Za-z]*\\.\", abbrv_dataset.iloc[index]['ingredients']):\n",
    "        abbrv.add(element)\n",
    "    \n",
    "print(abbrv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimozione delle abbreviazioni in quanto possono essere dannose per il processo di tokenizzazione. Es pkg. ---> package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviations(ingredients_string):\n",
    "    __ABBREVIATIONS__ = {\n",
    "        'pkg.'  :   'package',\n",
    "        'tsb.'  :   'tablespoon',\n",
    "        'no.'   :   'number',\n",
    "        'pt.'   :   'pint',\n",
    "        'no.'   :   'number',\n",
    "        'gal.'  :   'gallon',\n",
    "        'tbsp.' :   'tablespoon',\n",
    "        'sq.'   :   'square',\n",
    "        'oz.'   :   'ounce',\n",
    "        'lb.'   :   'pound',\n",
    "        'qt.'   :   'quart',\n",
    "        'c.'    :   'cup',\n",
    "        'tsp.'  :   'teaspoon'\n",
    "    }\n",
    "    for item, value in __ABBREVIATIONS__.items():\n",
    "        ingredients_string = ingredients_string.lower().replace(item, value)\n",
    "    return ingredients_string\n",
    "\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = expand_abbreviations(dataset.iloc[index]['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting\n",
    "Il contenuto delle colonne 'ingredients' e 'step' verrà suddiviso in frasi. In precedenza i periodi contenuti nelle singole celle sono stati formattati in modo tale da renderli riconoscibili e facilmente suddivisibili in frasi ben separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 box powdered sugar.', '8 ounce soft butter.', '1 (8 ounce) peanut butter.', 'paraffin.', '12 ounce chocolate chips']\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['ingredients'] = (sent_tokenize(dataset.iloc[index]['ingredients']))\n",
    "print(dataset.iloc[10]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mix sugar, butter and peanut butter.', 'Roll into balls and place on cookie sheet.', 'Set in freezer for at least 30 minutes.', 'Melt chocolate chips and paraffin in double boiler.', 'Using a toothpick, dip balls 3/4 of way into chocolate chip and paraffin mixture to make them look like buckeyes.']\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['step'] = (sent_tokenize(dataset.iloc[index]['step']))\n",
    "print(dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimozione quantità doppie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_reg = r'\\d*\\s*\\(.*\\)'\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    for value in range(len(dataset.iloc[index]['ingredients'])):\n",
    "        elements = re.findall(dr_reg, dataset.iloc[index]['ingredients'][value])\n",
    "        for e in elements:\n",
    "            new_string = dataset.iloc[index]['ingredients'][value].replace(e,e[e.find(\"(\")+1: e.find(\")\")].strip())\n",
    "            dataset.iloc[index]['ingredients'][value] = new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Nella colonna 'step' troviamo una serie di passaggi da compiere per creare la ricetta. Questi passaggi sono scritti in linguaggio naturale e possono essere semplificati rimuovendo delle parole dette stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in swr_dataset\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "swr_dataset = dataset.copy(deep=True)\n",
    "\n",
    "tk = lambda x,st: ' '.join([w for w in x if w not in st])\n",
    "for index in range(len(dataset)):\n",
    "    swr_dataset.iloc[index]['step'] = [tk(word_tokenize(sent), stop_words) for sent in dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming e Lemming\n",
    "Questi due processi potrebbero portare valore all'analisi del dominio. Il codice per entrambi viene proposto qui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in lem_dataset\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_dataset = swr_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(swr_dataset)):\n",
    "    lem_dataset.iloc[index]['step'] = stem_sent = [' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(sent)]) for sent in swr_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in stm_dataset\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stm_dataset = lem_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(lem_dataset)):\n",
    "    stm_dataset.iloc[index]['step'] = [' '.join([stemmer.stem(w) for w in word_tokenize(sent)]) for sent in lem_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frasi originali\\n\")\n",
    "print(dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word removal\\n\")\n",
    "print(swr_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word e lemming\\n\")\n",
    "print(lem_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word lemming e stemming\\n\")\n",
    "print(stm_dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entityt Extraction, Relation Extraction e POS tagging con SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dataset = dataset.copy(deep=True)\n",
    "       \n",
    "for index in range(len(dataset)):\n",
    "    spacy_dataset.iloc[index]['ingredients'] = [NLP(element) for element in dataset.iloc[index]['ingredients']]\n",
    "    spacy_dataset.iloc[index]['step'] = [NLP(element) for element in dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pprintsp__(dataset, index, column, value, extend=False):\n",
    "    displacy.render(dataset.iloc[index][column][value], style='dep')\n",
    "    print(dataset.iloc[index][column][value])\n",
    "    displacy.render(dataset.iloc[index][column][value], style='ent')\n",
    "    if extend:\n",
    "        for token in dataset.iloc[index][column][value]:\n",
    "            print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "def pprint_spacyd_all(dataset, index, column, extend=False):\n",
    "    for v in range(len(dataset.iloc[index][column])):\n",
    "        __pprintsp__(dataset, index, column, v, extend)\n",
    "            \n",
    "def pprint_spacyd(dataset, index, column, value, extend=False):\n",
    "    __pprintsp__(dataset, index, column, value, extend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi degli ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe:\n",
    "    def __init__(self, title, index, ingredients_str, steps_str):\n",
    "        self.title = title\n",
    "        self.ingredients = []\n",
    "        self.ing_str = ingredients_str\n",
    "        self.stp_str = steps_str\n",
    "        self.steps = []\n",
    "        self.idx = index\n",
    "    \n",
    "    def add_ing(self, ing):\n",
    "        self.ingredients.append(ing)\n",
    "        \n",
    "    def __str__(self):\n",
    "        _ing_s = '\\n'.join([ing.short_srt() for ing in self.ingredients])\n",
    "        _stp_s = '\\n'.join(str(step) for step in self.steps)\n",
    "        _header_ing = f\"|{'NOME':<40s}|{'PROPRIETÀ':<20s}|{'QUANTITÀ':<11s}|{'TIPO':<21s}|\"\n",
    "        _header_ing_l = f\"+{'-'*40}+{'-'*20}+{'-'*11}+{'-'*21}+\"\n",
    "        _header_step = f\"|{'N°':<2s}|{'AZIONE':<15s}|{'STRUMENTI':<20s}|{'INGREDIENTI':<55s}|\"\n",
    "        _header_step_l = f\"+{'-'*2}+{'-'*15}+{'-'*20}+{'-'*55}|\"\n",
    "        return f\"TITOLO: {self.title}\\n\\n{_header_ing}\\n{_header_ing_l}\\n{_ing_s}\\n{_header_ing_l}\\n\\n{_header_step}\\n{_header_step_l}\\n{_stp_s}\\n{_header_step_l}\"\n",
    "    \n",
    "    def print_unstructured(self):\n",
    "        return f\"{self.title}\\n\\n{self.ing_str}\\n\\n{self.stp_str}\"\n",
    "    \n",
    "    \n",
    "class Step:\n",
    "    def __init__(self, step_str, step_no, action, ins, ing):\n",
    "        self.action = action\n",
    "        self.ins = ins\n",
    "        self.ing = ing\n",
    "        self.step_str = step_str\n",
    "        self.step_no = step_no\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"|{self.step_no:<2d}|{self.action:<15s}|{','.join(self.ins):20s}|{','.join(self.ing):55s}|\"\n",
    "\n",
    "    \n",
    "    \n",
    "class Ingredient:\n",
    "    def __init__(self, full_text):\n",
    "        self.name=\"\"\n",
    "        self.ing_cat_id = None\n",
    "        self.size=\"\"\n",
    "        self.quantity= 0.0\n",
    "        self.size=\"\"\n",
    "        self.adj=[]\n",
    "        self.original=full_text\n",
    "    def __str__(self):\n",
    "        return f\"Name: {self.name}\\nAjdectives: {self.adj}\\nQuantity: {self.quantity:4f} {self.size}\\nOriginal: {self.original}\"\n",
    "\n",
    "    def short_srt(self):\n",
    "        return f\"|{self.name:<40s}|{','.join(self.adj):<20s}|{self.quantity:^11.2f}|{self.size:<21s}|\"\n",
    "    \n",
    "    def set_id(self, identifier):\n",
    "        self.ing_cat_id = identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione elenco ingredienti e ricette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_food(word):\n",
    "    syns = wn.synsets(str(word), pos = wn.NOUN)\n",
    "    for syn in syns:\n",
    "        if 'food' in syn.lexname():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def cls_w(word):\n",
    "    return word.strip().lower()\n",
    "\n",
    "def parse_quantity(qt):\n",
    "    if qt == \"\":\n",
    "        return 1\n",
    "    \n",
    "    partial_sum = 0.0\n",
    "    backtrack_multiplier = 1.0\n",
    "    \n",
    "    for elem in qt.split(\" \"):\n",
    "        try:\n",
    "        \n",
    "            if ('/' in elem) and (partial_sum == 0.0):\n",
    "                parsed = elem.split(\"/\")\n",
    "                backtrack_multiplier = float(parsed[0])/float(parsed[1])\n",
    "\n",
    "            elif ('/' in elem) and (partial_sum != 0.0):\n",
    "                parsed = elem.split(\"/\")\n",
    "                partial_sum += (float(parsed[0])/float(parsed[1]))\n",
    "\n",
    "            else:\n",
    "                partial_sum += float(elem)\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "    if(partial_sum != 0.0):\n",
    "        partial_sum *= backtrack_multiplier\n",
    "    else:\n",
    "        partial_sum = backtrack_multiplier\n",
    "        \n",
    "    return partial_sum\n",
    "            \n",
    "        \n",
    "\n",
    "#Root generation\n",
    "\n",
    "__SIZES__ = [\n",
    "       'package',\n",
    "       'tablespoon',\n",
    "       'number',\n",
    "       'pint',\n",
    "       'number',\n",
    "       'gallon',\n",
    "       'tablespoon',\n",
    "       'square',\n",
    "       'ounce',\n",
    "       'pound',\n",
    "       'quart',\n",
    "       'cup',\n",
    "       'teaspoon',\n",
    "       'can'\n",
    "]\n",
    "\n",
    "\n",
    "all_recipes = []\n",
    "    \n",
    "for index in range(len(spacy_dataset)):\n",
    "    \n",
    "    recipe = Recipe(\n",
    "        spacy_dataset.iloc[index]['title'], \n",
    "        index, \n",
    "        \" \".join(dataset.iloc[index]['ingredients']),\n",
    "        \" \".join(dataset.iloc[index]['step'])       \n",
    "    )\n",
    "    \n",
    "    for element in spacy_dataset.iloc[index]['ingredients']:\n",
    "\n",
    "        ingredient = element\n",
    "\n",
    "        IGN = Ingredient(ingredient.text)\n",
    "        ROOT_NODE = [token for token in ingredient if token.dep_ == 'ROOT'][0]\n",
    "        \n",
    "\n",
    "        if (ROOT_NODE.pos == VERB):\n",
    "                IGN.adj.append(cls_w(ROOT_NODE.text))\n",
    "        else:\n",
    "                IGN.name = cls_w(ROOT_NODE.text)\n",
    "                \n",
    "        stack = [element for element in ROOT_NODE.children]\n",
    "\n",
    "        unparsed_quantity = \"\"\n",
    "        \n",
    "        while len(stack)!=0:\n",
    "            CURRENT_NODE = stack.pop()\n",
    "\n",
    "            if (CURRENT_NODE.pos == X) or (CURRENT_NODE.pos == NUM):\n",
    "                    unparsed_quantity = CURRENT_NODE.text + \" \" + unparsed_quantity\n",
    "            else:\n",
    "                if CURRENT_NODE.text.lower() in  __SIZES__:\n",
    "                    IGN.size += \" \" + cls_w(CURRENT_NODE.text)\n",
    "\n",
    "                if ((CURRENT_NODE.dep_ == 'compound') or (CURRENT_NODE.dep_ == 'dobj') or (CURRENT_NODE.dep_ == 'pobj')) and is_food(CURRENT_NODE.text) and (CURRENT_NODE.text.lower() not in  __SIZES__):\n",
    "                    IGN.name = CURRENT_NODE.text + \" \" + cls_w(IGN.name)\n",
    "\n",
    "                if (CURRENT_NODE.dep_ == 'amod') or (CURRENT_NODE.dep_ == 'appos'):\n",
    "                    if is_food(CURRENT_NODE.text):\n",
    "                        IGN.name = CURRENT_NODE.text + \" \" + cls_w(IGN.name)\n",
    "                    else:\n",
    "                        IGN.adj.append(cls_w(CURRENT_NODE.text))         \n",
    "\n",
    "            stack += [element for element in CURRENT_NODE.children]\n",
    "        \n",
    "        if IGN.name.strip() != \"\":\n",
    "            IGN.quantity = parse_quantity(cls_w(unparsed_quantity))\n",
    "            IGN.name = cls_w(IGN.name)\n",
    "            recipe.add_ing(IGN)\n",
    "    all_recipes.append(recipe)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in all_recipes[0:2]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi degli step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_all_words = set()\n",
    "\n",
    "for element in all_recipes:\n",
    "    for ingredient in element.ingredients:\n",
    "        for e in ingredient.name.split(\" \"):\n",
    "            ingredients_all_words.add(cls_w(e))\n",
    "\n",
    "ingredients_all_words.add('potato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_verb(word):\n",
    "    ss = wn.synsets(word)\n",
    "    for s in ss:\n",
    "        if s.pos() == 'v':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def combine_action(action_token):\n",
    "    action = cls_w(action_token.text)\n",
    "    for child in action_token.children:\n",
    "        if child.dep_ == 'prt' or child.dep_ == 'compound':\n",
    "            action += \" \" + cls_w(child.text)\n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_instrument(token):\n",
    "    stack = [element for element in token.children]\n",
    "    instrument = \"\"\n",
    "    found_main = False\n",
    "    main_node = None\n",
    "\n",
    "    while stack:\n",
    "        s_element = stack.pop()\n",
    "        if s_element.pos == NOUN and (cls_w(s_element.text) not in ingredients_all_words) and (not found_main) and (s_element.text not in forbidden_instruments):\n",
    "            if \"ed\" != s_element.text[-2:] and \"ing\" != s_element.text[-3:]:\n",
    "                instrument += cls_w(s_element.text)\n",
    "                found_main = True\n",
    "                main_node = s_element\n",
    "        elif s_element.dep_ == 'compound' and main_node == s_element.head and (cls_w(s_element.text) not in ingredients_all_words):\n",
    "            instrument = cls_w(s_element.text) + \" \" + instrument\n",
    "            main_node = s_element\n",
    "            \n",
    "        stack += [element for element in s_element.children]\n",
    "    return instrument\n",
    "\n",
    "\n",
    "forbidden_actions = ['done', 'will', 'be']\n",
    "forbidden_instruments = ['top']\n",
    "PREPS = ['with', 'in', 'on', 'of', 'using', 'into', 'use']\n",
    "\n",
    "def get_ingredients(recipe):\n",
    "    return {ing.name for ing in recipe.ingredients}\n",
    "\n",
    "def find_ingredients(ings, step):\n",
    "     \n",
    "    ing = set()\n",
    "    for word in step.replace(',','').replace('.','').replace('(','').replace(')','').replace('*','').split(\" \"):\n",
    "        word = cls_w(word)\n",
    "        for el in ings:\n",
    "            if re.findall(f\"(\\\\s{word}\\\\s)|({word}\\\\s)|(\\\\s{word})\", el) or el == word:\n",
    "                ing.add(el)                \n",
    "    return ing\n",
    "\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    step_counter = 1\n",
    "    ing_s = get_ingredients(all_recipes[index])\n",
    "    for value in range(len(dataset.iloc[index]['step'])):\n",
    "        for element in dataset.iloc[index]['step'][value].split(\";\"):\n",
    "               \n",
    "            step = NLP(element.replace(\"°\", '').strip())\n",
    "            \n",
    "            FINAL_ACTION = \"\"\n",
    "            _hidden_verbs = []\n",
    "            _found_action = False\n",
    "            found_instruments = []\n",
    "            found_ingredients = []\n",
    "            \n",
    "            for TOKEN in step:\n",
    "                if (\"ed\" != TOKEN.text[-2:]) and TOKEN.text not in forbidden_actions and (\"ing\" != TOKEN.text[-3:]):\n",
    "                    if not _found_action:\n",
    "                        if (TOKEN.pos == VERB):\n",
    "                            FINAL_ACTION = combine_action(TOKEN)\n",
    "                            _found_action = True\n",
    "                        elif hidden_verb(TOKEN.text):\n",
    "                            _hidden_verbs.append(combine_action(TOKEN))\n",
    "                            \n",
    "                if TOKEN.text in PREPS:\n",
    "                    instrument = find_instrument(TOKEN)\n",
    "                    if instrument != \"\":\n",
    "                        found_instruments.append(cls_w(instrument))\n",
    "                        \n",
    "                      \n",
    "            if FINAL_ACTION == \"\" and _hidden_verbs:\n",
    "                FINAL_ACTION = _hidden_verbs[0]\n",
    "                \n",
    "            if FINAL_ACTION != \"\":\n",
    "                found_step = Step(element, step_counter, cls_w(FINAL_ACTION), found_instruments, find_ingredients(ing_s, step.text))\n",
    "                all_recipes[index].steps.append(found_step)\n",
    "                step_counter += 1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open('../dump.txt', 'w') as f:\n",
    "    for i, index in enumerate(all_recipes):\n",
    "        f.write(index.print_unstructured() + '\\n')\n",
    "        f.write(f\"[{i}]\" + str(index) + '\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAZIONE DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione indici per ricette e ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_ingredients = {}\n",
    "ingredients_idx = {}\n",
    "macro_steps = {}\n",
    "steps_idx = {}\n",
    "macro_instruments = {}\n",
    "instruments_idx = {}\n",
    "\n",
    "for element in all_recipes:\n",
    "    for ingredient in element.ingredients:\n",
    "        if ingredient.name in macro_ingredients:\n",
    "            macro_ingredients[ingredient.name] += 1\n",
    "        else:\n",
    "            macro_ingredients[ingredient.name] = 1\n",
    "            \n",
    "for index, key in enumerate(macro_ingredients):\n",
    "    ingredients_idx[key] = index\n",
    "    print(f\"{key:>50s} {macro_ingredients[key]:<3d}\")\n",
    "\n",
    "\n",
    "for element in all_recipes:\n",
    "    for step in element.steps:\n",
    "        if step.action in macro_steps:\n",
    "            macro_steps[step.action] += 1\n",
    "        else:\n",
    "            macro_steps[step.action] = 1\n",
    "            \n",
    "        for ins in step.ins:\n",
    "            if ins in macro_instruments:\n",
    "                macro_instruments[ins] += 1\n",
    "            else:\n",
    "                macro_instruments[ins] = 1\n",
    "            \n",
    "            \n",
    "print(100*'-')\n",
    "\n",
    "for index, key in enumerate(macro_steps):\n",
    "    steps_idx[key] = index\n",
    "    print(f\"{key:>50s} {macro_steps[key]:<3d}\")\n",
    "    \n",
    "print(100*'-')\n",
    "\n",
    "for index, key in enumerate(macro_instruments):\n",
    "    instruments_idx[key] = index\n",
    "    print(f\"{key:>50s} {macro_instruments[key]:<3d}\")\n",
    "    \n",
    "print(instruments_idx)\n",
    "print(steps_idx)\n",
    "print(ingredients_idx)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_step = [\n",
    "    'Pour gravy and cream of mushroom soup over chicken',\n",
    "    'Put stuffing on top of chicken and gravy',\n",
    "    'Empty Cool Whip into a bowl',\n",
    "    'Using a toothpick, dip balls 3/4 of way into chocolate',\n",
    "    'Stir in barbecue sauce.',\n",
    "    'Pour into a 9 x 13-inch pan.',\n",
    "    'Butter a square pan, 8 x 8 x 2-inches.',\n",
    "    'Bake 45 minutes in a 350° oven.',\n",
    "    'Bake in a 350° oven for about 20 minutes or in microwave until cheese',\n",
    "    'Bake in 375° oven for 10 to 12 minutes or until golden brown.'\n",
    "]\n",
    "\n",
    "for element in wrong_step:\n",
    "    displacy.render(NLP(element), style='dep')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ING_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS ingredients;\n",
    "CREATE TABLE ingredients(\n",
    "id INT PRIMARY KEY,\n",
    "name VARCHAR(100) NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(ING_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_ingredients.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for ing in ingredients_idx:\n",
    "        ING_DB_CREATE = ING_DB_CREATE + f\"INSERT INTO ingredients VALUES ({ingredients_idx[ing]},\\\"{ing}\\\");\\n\"\n",
    "    ddl_file.write(ING_DB_CREATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL ricette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REC_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS recipes;\n",
    "CREATE TABLE recipes(\n",
    "id INT PRIMARY KEY,\n",
    "titolo VARCHAR(100) NOT NULL,\n",
    "preparazione_nstr VARCHAR(500),\n",
    "ingredienti_nstr VARCHAR(500)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(REC_DB_CREATE)\n",
    "\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_recipes.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for rec in all_recipes:\n",
    "        \n",
    "        corrected_ing = rec.ing_str.replace(\"\\'\",\"\\'\\'\")\n",
    "        corrected_stp = rec.stp_str.replace(\"\\'\",\"\\'\\'\")\n",
    "        corrected_tit = rec.title.replace(\"\\'\",\"\\'\\'\")\n",
    "        \n",
    "        REC_DB_CREATE = REC_DB_CREATE + f\"INSERT INTO recipes (id, titolo, ingredienti_nstr, preparazione_nstr) VALUES ({rec.idx},\\'{corrected_tit}\\',\\'{corrected_ing}\\',\\'{corrected_stp}\\');\\n\"\n",
    "    ddl_file.write(REC_DB_CREATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione DDL tabella relazione Ricette <--> Ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CON_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS contains;\n",
    "CREATE TABLE contains(\n",
    "id INT NOT NULL AUTO_INCREMENT,\n",
    "misura VARCHAR(50),\n",
    "quantita FLOAT,\n",
    "proprieta VARCHAR(100),\n",
    "recipeID int NOT NULL,\n",
    "ingredientID int NOT NULL,\n",
    "PRIMARY KEY (id),\n",
    "FOREIGN KEY (recipeID) REFERENCES recipes(id),\n",
    "FOREIGN KEY (ingredientID) REFERENCES ingredients(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(CON_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_contains.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for r in all_recipes:\n",
    "        for i in r.ingredients:\n",
    "            if i.name.strip() != \"\":\n",
    "                adj = ', '.join(i.adj)\n",
    "                data = f\"(\\'{i.size}\\',{i.quantity},\\'{adj}\\',{r.idx}, {ingredients_idx[i.name]})\"\n",
    "                CON_DB_CREATE = CON_DB_CREATE + f\"INSERT INTO contains (misura, quantita, proprieta, recipeID, ingredientID) VALUES {data};\\n\"\n",
    "    ddl_file.write(CON_DB_CREATE)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fef19a476ade33c91668e3897e6fa668c17f7c4a539d9a83d441085d42ee0ef"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
