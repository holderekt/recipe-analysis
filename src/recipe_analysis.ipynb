{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricette definite in un formato semistrutturato, raggruppamento degli step e ingredienti in un unica stringa, funzioni di utilità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni util e import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.sem import relextract\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.corpus import conll2000\n",
    "from spacy.symbols import X, NUM, VERB, NOUN\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('conll2000')\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "NLP = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "def string_recipe(i):\n",
    "    return dataset.iloc[i]['title'] + \"\\n\\n\" + dataset.iloc[i]['ingredients'] + \"\\n\\n\" + dataset.iloc[i]['step'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = \".\\n\".join(literal_eval(dataset.iloc[index]['ingredients']))\n",
    "    dataset.iloc[index]['step'] = \" \".join(literal_eval(dataset.iloc[index]['step']))\n",
    "\n",
    "display(dataset.head())\n",
    "print(string_recipe(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione delle abbreviazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase iniziale di ritrovamento del set di abbreviazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrv_dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "abbrv = set()\n",
    "for index in range(len(dataset)):\n",
    "    abbrv_dataset.iloc[index]['ingredients'] = \" \".join(literal_eval(abbrv_dataset.iloc[index]['ingredients']))\n",
    "    for element in re.findall(r\"[A-Za-z]*\\.\", abbrv_dataset.iloc[index]['ingredients']):\n",
    "        abbrv.add(element)\n",
    "    \n",
    "print(abbrv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimozione delle abbreviazioni in quanto possono essere dannose per il processo di tokenizzazione. Es pkg. ---> package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviations(ingredients_string):\n",
    "    __ABBREVIATIONS__ = {\n",
    "        'pkg.'  :   'package',\n",
    "        'tsb.'  :   'tablespoon',\n",
    "        'no.'   :   'number',\n",
    "        'pt.'   :   'pint',\n",
    "        'no.'   :   'number',\n",
    "        'gal.'  :   'gallon',\n",
    "        'tbsp.' :   'tablespoon',\n",
    "        'sq.'   :   'square',\n",
    "        'oz.'   :   'ounce',\n",
    "        'lb.'   :   'pound',\n",
    "        'qt.'   :   'quart',\n",
    "        'c.'    :   'cup',\n",
    "        'tsp.'  :   'teaspoon'\n",
    "    }\n",
    "    for item, value in __ABBREVIATIONS__.items():\n",
    "        ingredients_string = ingredients_string.lower().replace(item, value)\n",
    "    return ingredients_string\n",
    "\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = expand_abbreviations(dataset.iloc[index]['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting\n",
    "Il contenuto delle colonne 'ingredients' e 'step' verrà suddiviso in frasi. In precedenza i periodi contenuti nelle singole celle sono stati formattati in modo tale da renderli riconoscibili e facilmente suddivisibili in frasi ben separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['ingredients'] = (sent_tokenize(dataset.iloc[index]['ingredients']))\n",
    "print(dataset.iloc[10]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['step'] = (sent_tokenize(dataset.iloc[index]['step']))\n",
    "print(dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimozione quantità doppie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_reg = r'\\d*\\s*\\(.*\\)'\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    for value in range(len(dataset.iloc[index]['ingredients'])):\n",
    "        elements = re.findall(dr_reg, dataset.iloc[index]['ingredients'][value])\n",
    "        for e in elements:\n",
    "            new_string = dataset.iloc[index]['ingredients'][value].replace(e,e[e.find(\"(\")+1: e.find(\")\")].strip())\n",
    "            dataset.iloc[index]['ingredients'][value] = new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Nella colonna 'step' troviamo una serie di passaggi da compiere per creare la ricetta. Questi passaggi sono scritti in linguaggio naturale e possono essere semplificati rimuovendo delle parole dette stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in swr_dataset\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "swr_dataset = dataset.copy(deep=True)\n",
    "\n",
    "tk = lambda x,st: ' '.join([w for w in x if w not in st])\n",
    "for index in range(len(dataset)):\n",
    "    swr_dataset.iloc[index]['step'] = [tk(word_tokenize(sent), stop_words) for sent in dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming e Lemming\n",
    "Questi due processi potrebbero portare valore all'analisi del dominio. Il codice per entrambi viene proposto qui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in lem_dataset\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_dataset = swr_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(swr_dataset)):\n",
    "    lem_dataset.iloc[index]['step'] = stem_sent = [' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(sent)]) for sent in swr_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in stm_dataset\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stm_dataset = lem_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(lem_dataset)):\n",
    "    stm_dataset.iloc[index]['step'] = [' '.join([stemmer.stem(w) for w in word_tokenize(sent)]) for sent in lem_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frasi originali\\n\")\n",
    "print(dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word removal\\n\")\n",
    "print(swr_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word e lemming\\n\")\n",
    "print(lem_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word lemming e stemming\\n\")\n",
    "print(stm_dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entityt Extraction, Relation Extraction e POS tagging con SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dataset = dataset.copy(deep=True)\n",
    "       \n",
    "for index in range(len(dataset)):\n",
    "    spacy_dataset.iloc[index]['ingredients'] = [NLP(element) for element in dataset.iloc[index]['ingredients']]\n",
    "    spacy_dataset.iloc[index]['step'] = [NLP(element) for element in dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pprintsp__(dataset, index, column, value, extend=False):\n",
    "    displacy.render(dataset.iloc[index][column][value], style='dep')\n",
    "    print(dataset.iloc[index][column][value])\n",
    "    displacy.render(dataset.iloc[index][column][value], style='ent')\n",
    "    if extend:\n",
    "        for token in dataset.iloc[index][column][value]:\n",
    "            print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "def pprint_spacyd_all(dataset, index, column, extend=False):\n",
    "    for v in range(len(dataset.iloc[index][column])):\n",
    "        __pprintsp__(dataset, index, column, v, extend)\n",
    "            \n",
    "def pprint_spacyd(dataset, index, column, value, extend=False):\n",
    "    __pprintsp__(dataset, index, column, value, extend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi degli ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recipe:\n",
    "    def __init__(self, title, index, ingredients_str, steps_str):\n",
    "        self.title = title\n",
    "        self.ingredients = []\n",
    "        self.ing_str = ingredients_str\n",
    "        self.stp_str = steps_str\n",
    "        self.steps = []\n",
    "        self.idx = index\n",
    "    \n",
    "    def add_ing(self, ing):\n",
    "        self.ingredients.append(ing)\n",
    "        \n",
    "    def __str__(self):\n",
    "        _ing_s = '\\n'.join([ing.short_srt() for ing in self.ingredients])\n",
    "        _stp_s = '\\n'.join(str(step) for step in self.steps)\n",
    "        return f\"Title: {self.title}\\n{_ing_s}\\nSteps:\\n{_stp_s}\"\n",
    "    \n",
    "    \n",
    "    \n",
    "class Step:\n",
    "    def __init__(self, step_str, step_no, action, ins):\n",
    "        self.action = action\n",
    "        self.ins = ins\n",
    "        self.des = \"\"\n",
    "        self.step_str = step_str\n",
    "        self.step_no = step_no\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.step_no:<3d} {self.action:<15s} [{','.join(self.ins):20s}] ({self.step_str})\"\n",
    "\n",
    "    \n",
    "    \n",
    "class Ingredient:\n",
    "    def __init__(self, full_text):\n",
    "        self.name=\"\"\n",
    "        self.ing_cat_id = None\n",
    "        self.size=\"\"\n",
    "        self.quantity=\"\"\n",
    "        self.size=\"\"\n",
    "        self.adj=[]\n",
    "        self.original=full_text\n",
    "    def __str__(self):\n",
    "        return f\"Name: {self.name}\\nAjdectives: {self.adj}\\nQuantity: {self.quantity} {self.size}\\nOriginal: {self.original}\"\n",
    "\n",
    "    def short_srt(self):\n",
    "        return f\"{self.name} ({self.adj}) {self.quantity} {self.size}\"\n",
    "    \n",
    "    def set_id(self, identifier):\n",
    "        self.ing_cat_id = identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_food(word):\n",
    "    syns = wn.synsets(str(word), pos = wn.NOUN)\n",
    "    for syn in syns:\n",
    "        if 'food' in syn.lexname():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#Root generation\n",
    "\n",
    "__SIZES__ = [\n",
    "       'package',\n",
    "       'tablespoon',\n",
    "       'number',\n",
    "       'pint',\n",
    "       'number',\n",
    "       'gallon',\n",
    "       'tablespoon',\n",
    "       'square',\n",
    "       'ounce',\n",
    "       'pound',\n",
    "       'quart',\n",
    "       'cup',\n",
    "       'teaspoon',\n",
    "       'can'\n",
    "]\n",
    "\n",
    "\n",
    "all_recipes = []\n",
    "    \n",
    "for index in range(len(spacy_dataset)):\n",
    "    \n",
    "    recipe = Recipe(\n",
    "        spacy_dataset.iloc[index]['title'], \n",
    "        index, \n",
    "        \" \".join(dataset.iloc[index]['ingredients']),\n",
    "        \" \".join(dataset.iloc[index]['step'])       \n",
    "    )\n",
    "    \n",
    "    for element in spacy_dataset.iloc[index]['ingredients']:\n",
    "\n",
    "        ingredient = element\n",
    "\n",
    "        IGN = Ingredient(ingredient.text)\n",
    "        ROOT_NODE = [token for token in ingredient if token.dep_ == 'ROOT'][0]\n",
    "        \n",
    "\n",
    "        if (ROOT_NODE.pos == VERB):\n",
    "                IGN.adj.append(ROOT_NODE.text)\n",
    "        else:\n",
    "                IGN.name = ROOT_NODE.text\n",
    "                \n",
    "        stack = [element for element in ROOT_NODE.children]\n",
    "\n",
    "        while len(stack)!=0:\n",
    "            CURRENT_NODE = stack.pop()\n",
    "\n",
    "            if (CURRENT_NODE.pos == X) or (CURRENT_NODE.pos == NUM):\n",
    "                    IGN.quantity += \" \" + CURRENT_NODE.text\n",
    "            else:\n",
    "                if CURRENT_NODE.text.lower() in  __SIZES__:\n",
    "                    IGN.size += \" \" + CURRENT_NODE.text\n",
    "\n",
    "                if ((CURRENT_NODE.dep_ == 'compound') or (CURRENT_NODE.dep_ == 'dobj') or (CURRENT_NODE.dep_ == 'pobj')) and is_food(CURRENT_NODE.text) and (CURRENT_NODE.text.lower() not in  __SIZES__):\n",
    "                    IGN.name = CURRENT_NODE.text + \" \" + IGN.name\n",
    "\n",
    "                if (CURRENT_NODE.dep_ == 'amod') or (CURRENT_NODE.dep_ == 'appos'):\n",
    "                    if is_food(CURRENT_NODE.text):\n",
    "                        IGN.name = CURRENT_NODE.text + \" \" + IGN.name\n",
    "                    else:\n",
    "                        IGN.adj.append(CURRENT_NODE.text)         \n",
    "\n",
    "            stack += [element for element in CURRENT_NODE.children]\n",
    "            \n",
    "        recipe.add_ing(IGN)\n",
    "    all_recipes.append(recipe)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi degli step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPS = ['with', 'in', 'on', 'of', 'using', 'an']\n",
    "\n",
    "#Questa funzione potrebbe essere migliorata in mille modi, per ora me la faccio bastare\n",
    "def find_instrument(child):\n",
    "    if child.dep_ == 'prep' and child.text in PREPS:\n",
    "        for subchild in child.children:\n",
    "            if subchild.dep_ == 'pobj' and not is_food(subchild.text):\n",
    "                for subsubchild in subchild.children:\n",
    "                    if subsubchild.dep_ == 'prep' and subsubchild.text in PREPS:\n",
    "                        if subsubchild.dep_ == 'pobj' and not is_food(subchild.text):\n",
    "                            ins = subsubchild.text\n",
    "                            for conn in subsubchild.children:\n",
    "                                if conn.dep_ == 'compound':\n",
    "                                    ins = conn.text + ' ' + ins\n",
    "                            return ins\n",
    "                    else:\n",
    "                        ins = subchild.text\n",
    "                        for conn in subchild.children:\n",
    "                            if conn.dep_ == 'compound':\n",
    "                                ins = conn.text + ' ' + ins\n",
    "                        return ins\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Questa funzione sono riuscito a farla leggermente meglio e funziona discretamente bene, \n",
    "# potrebbe essere fatta qualche miglioria ma tutto sommato fa il suo dovere\n",
    "def find_ingredient(child):\n",
    "    ing_list = []\n",
    "    ing = child.text\n",
    "    for subchild in child.children:\n",
    "        if subchild.dep_ == 'amod'and is_food(subchild.text):\n",
    "            ing = subchild.text + ' ' + ing\n",
    "    ing_list.append(ing)\n",
    "    for subchild in child.children:\n",
    "        if subchild.dep_ == 'conj' and is_food(subchild.text):\n",
    "            ing_list.append(find_ingredient(subchild).pop())\n",
    "    return ing_list\n",
    "\n",
    "def hidden_verb(word):\n",
    "    ss = wn.synsets(word)\n",
    "    for s in ss:\n",
    "        if s.pos() == 'v':\n",
    "            return True\n",
    "    return False\n",
    "                \n",
    "actions = set()\n",
    "instruments = set()\n",
    "ingredients_conn = set()\n",
    "\n",
    "for index in range(len(spacy_dataset)):\n",
    "    print(index)\n",
    "    for doc in spacy_dataset.iloc[index]['step']:\n",
    "        print(\"\\n\\n\")\n",
    "        print(doc)\n",
    "        #displacy.render(doc, style='dep')\n",
    "        for token in doc:\n",
    "            #Cerco i verbi\n",
    "            if token.pos_ == 'VERB' or hidden_verb(token.text):\n",
    "                #AZIONE TROVATA (per ora sono tutti i verbi, effettuare un controllo ulteriore?)\n",
    "                act = token.text\n",
    "                actions.add(act)\n",
    "\n",
    "                for child in token.children:\n",
    "                    #Tra tutti i child del verbo mi cerco lo strumento tramite la funzione find_instrument\n",
    "                    if find_instrument(child):\n",
    "                        ins = find_instrument(child)\n",
    "                        print(ins)\n",
    "                        instruments.add(ins)\n",
    "                    #Se un child ha dipendenza dobj è un ingrediente e chiamo la funzione find_ingredient che mi ritorna una lista di ingredienti, aggiungo singolarmente ognuno di essi al set\n",
    "                    if child.dep_ == 'dobj' and is_food(child.text):\n",
    "                        ing_list = find_ingredient(child)\n",
    "                        for ing in ing_list:\n",
    "                            print(ing)\n",
    "                            ingredients_conn.add(ing)\n",
    "#print(actions)\n",
    "#print(instruments)\n",
    "print(ingredients_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_verb(word):\n",
    "    ss = wn.synsets(word)\n",
    "    for s in ss:\n",
    "        if s.pos() == 'v':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def combine_action(action_token):\n",
    "    action = action_token.text\n",
    "    for child in action_token.children:\n",
    "        if child.dep_ == 'prt' or child.dep_ == 'compound':\n",
    "            action += \" \" + child.text\n",
    "    return action\n",
    "\n",
    "def find_instrument(token):\n",
    "    stack = [element for element in token.children]\n",
    "    instrument = \"\"\n",
    "    found_main = False\n",
    "    main_node = None\n",
    "\n",
    "    while stack:\n",
    "        s_element = stack.pop()\n",
    "        if s_element.pos == NOUN and (s_element.text not in ingredients_all_words) and (not found_main) and (s_element.text not in forbidden_instruments):\n",
    "            instrument += s_element.text\n",
    "            found_main = True\n",
    "            main_node = s_element\n",
    "        elif s_element.dep_ == 'compound' and main_node == s_element.head:\n",
    "            instrument = s_element.text + \" \" + instrument\n",
    "            main_node = s_element\n",
    "\n",
    "        childrens = [element for element in s_element.children]\n",
    "        stack += [element for element in s_element.children]\n",
    "    return instrument\n",
    "\n",
    "\n",
    "forbidden_actions = ['done', 'will', 'be']\n",
    "forbidden_instruments = ['top']\n",
    "PREPS = ['with', 'in', 'on', 'of', 'using', 'an', 'a']\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    step_counter = 1\n",
    "    for value in range(len(dataset.iloc[index]['step'])):\n",
    "        for element in dataset.iloc[index]['step'][value].split(\";\"):\n",
    "            \n",
    "            \n",
    "            #print(\"\\n\" , element)\n",
    "            #print(\"Instruments: \")\n",
    "            step = NLP(element.strip())\n",
    "            \n",
    "            #displacy.render(step, style='dep')\n",
    "            FINAL_ACTION = \"\"\n",
    "            _hidden_verbs = []\n",
    "            _found_action = False\n",
    "            found_instruments = []\n",
    "            found_ingredients = []\n",
    "            \n",
    "            for TOKEN in step:\n",
    "                #AZIONE TROVATA (per ora sono tutti i verbi, effettuare un controllo ulteriore?)\n",
    "                if \"ed\" not in (TOKEN.text) and TOKEN.text not in forbidden_actions and \"ing\" not in (TOKEN.text):\n",
    "                    if not _found_action:\n",
    "                        if (TOKEN.pos == VERB):\n",
    "                            FINAL_ACTION = combine_action(TOKEN)\n",
    "                            _found_action = True\n",
    "                        elif hidden_verb(TOKEN.text):\n",
    "                            _hidden_verbs.append(combine_action(TOKEN))\n",
    "                            \n",
    "                if TOKEN.text in PREPS:\n",
    "                    #Tra tutti i child del verbo mi cerco lo strumento tramite la funzione find_instrument\n",
    "                    instrument = find_instrument(TOKEN)\n",
    "                    if instrument != \"\":\n",
    "                        found_instruments.append(instrument)\n",
    "                        \n",
    "                      \n",
    "            if FINAL_ACTION == \"\" and _hidden_verbs:\n",
    "                FINAL_ACTION = _hidden_verbs[0]\n",
    "                \n",
    "            if FINAL_ACTION != \"\":\n",
    "                found_step = Step(element, step_counter, FINAL_ACTION, found_instruments)\n",
    "                all_recipes[index].steps.append(found_step)\n",
    "                step_counter += 1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in all_recipes[11:100]:\n",
    "    print(index, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAZIONE DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione indici per ricette e ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_ingredients = {}\n",
    "ingredients_idx = {}\n",
    "\n",
    "for element in all_recipes:\n",
    "    for ingredient in element.ingredients:\n",
    "        if ingredient.name in macro_ingredients:\n",
    "            macro_ingredients[ingredient.name] += 1\n",
    "        else:\n",
    "            macro_ingredients[ingredient.name] = 1\n",
    "            \n",
    "for index, key in enumerate(macro_ingredients):\n",
    "    ingredients_idx[key] = index\n",
    "    #print(f\"{key:>50s} {macro_ingredients[key]:<3d}\")\n",
    "\n",
    "for key in ingredients_idx:\n",
    "    print(key.split(\" \"))\n",
    "ingredients_all_words = set([])\n",
    "for key in ingredients_idx:\n",
    "    for element in key.split(\" \"):\n",
    "        ingredients_all_words.add(element)\n",
    "print(ingredients_all_words)\n",
    "\n",
    "print('potato' in ingredients_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_recipes[0].ing_str)\n",
    "print(all_recipes[0].stp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ING_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS ingredients;\n",
    "CREATE TABLE ingredients(\n",
    "id INT PRIMARY KEY,\n",
    "name VARCHAR(100) NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(ING_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_ingredients.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for ing in ingredients_idx:\n",
    "        ING_DB_CREATE = ING_DB_CREATE + f\"INSERT INTO ingredients VALUES ({ingredients_idx[ing]},\\\"{ing}\\\");\\n\"\n",
    "    ddl_file.write(ING_DB_CREATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione DDL ricette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REC_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS recipes;\n",
    "CREATE TABLE recipes(\n",
    "id INT PRIMARY KEY,\n",
    "titolo VARCHAR(100) NOT NULL,\n",
    "preparazione_nstr VARCHAR(500),\n",
    "ingredienti_nstr VARCHAR(500)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(REC_DB_CREATE)\n",
    "\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_recipes.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for rec in all_recipes:\n",
    "        \n",
    "        corrected_ing = rec.ing_str.replace(\"\\'\",\"\\'\\'\")\n",
    "        corrected_stp = rec.stp_str.replace(\"\\'\",\"\\'\\'\")\n",
    "        corrected_tit = rec.title.replace(\"\\'\",\"\\'\\'\")\n",
    "        \n",
    "        REC_DB_CREATE = REC_DB_CREATE + f\"INSERT INTO recipes (id, titolo, ingredienti_nstr, preparazione_nstr) VALUES ({rec.idx},\\'{corrected_tit}\\',\\'{corrected_ing}\\',\\'{corrected_stp}\\');\\n\"\n",
    "    ddl_file.write(REC_DB_CREATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crezione DDL tabella relazione Ricette <--> Ingredienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CON_DB_CREATE = \"\"\"\n",
    "USE recipe_analysis;\n",
    "DROP TABLE IF EXISTS contains;\n",
    "CREATE TABLE contains(\n",
    "id INT NOT NULL AUTO_INCREMENT,\n",
    "misura VARCHAR(50),\n",
    "quantita VARCHAR(50),\n",
    "proprieta VARCHAR(100),\n",
    "recipeID int NOT NULL,\n",
    "ingredientID int NOT NULL,\n",
    "PRIMARY KEY (id),\n",
    "FOREIGN KEY (recipeID) REFERENCES recipes(id),\n",
    "FOREIGN KEY (ingredientID) REFERENCES ingredients(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(CON_DB_CREATE)\n",
    "\n",
    "\n",
    "with open(Path(\"../database/create_contains.sql\").resolve(), \"w\") as ddl_file:\n",
    "    for r in all_recipes:\n",
    "        for i in r.ingredients:\n",
    "            if i.name.strip() != \"\":\n",
    "                adj = ', '.join(i.adj)\n",
    "                data = f\"(\\'{i.size}\\',\\'{i.quantity}\\',\\'{adj}\\',{r.idx}, {ingredients_idx[i.name]})\"\n",
    "                CON_DB_CREATE = CON_DB_CREATE + f\"INSERT INTO contains (misura, quantita, proprieta, recipeID, ingredientID) VALUES {data};\\n\"\n",
    "    ddl_file.write(CON_DB_CREATE)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fef19a476ade33c91668e3897e6fa668c17f7c4a539d9a83d441085d42ee0ef"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
