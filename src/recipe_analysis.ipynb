{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricette definite in un formato semistrutturato, raggruppamento degli step e ingredienti in un unica stringa, funzioni di utilità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni util e import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.sem import relextract\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('conll2000')\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "NLP = spacy.load('en_core_web_md')\n",
    "\n",
    "\n",
    "def string_recipe(i):\n",
    "    return dataset.iloc[i]['title'] + \"\\n\\n\" + dataset.iloc[i]['ingredients'] + \"\\n\\n\" + dataset.iloc[i]['step'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = \".\\n\".join(literal_eval(dataset.iloc[index]['ingredients']))\n",
    "    dataset.iloc[index]['step'] = \" \".join(literal_eval(dataset.iloc[index]['step']))\n",
    "\n",
    "display(dataset.head())\n",
    "print(string_recipe(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione delle abbreviazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase iniziale di ritrovamento del set di abbreviazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrv_dataset = pd.read_csv(\n",
    "    Path(\"../data/test_dataset.csv\").resolve(), \n",
    "    index_col=[0], \n",
    "    names=[\"index\", \"title\",\"ingredients\",\"step\"], \n",
    "    usecols=[0,1,2,3]\n",
    "    )\n",
    "\n",
    "abbrv = set()\n",
    "for index in range(len(dataset)):\n",
    "    abbrv_dataset.iloc[index]['ingredients'] = \" \".join(literal_eval(abbrv_dataset.iloc[index]['ingredients']))\n",
    "    for element in re.findall(r\"[A-Za-z]*\\.\", abbrv_dataset.iloc[index]['ingredients']):\n",
    "        abbrv.add(element)\n",
    "    \n",
    "print(abbrv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimozione delle abbreviazioni in quanto possono essere dannose per il processo di tokenizzazione. Es pkg. ---> package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviations(ingredients_string):\n",
    "    __ABBREVIATIONS__ = {\n",
    "        'pkg.'  :   'package',\n",
    "        'tsb.'  :   'tablespoon',\n",
    "        'no.'   :   'number',\n",
    "        'pt.'   :   'pint',\n",
    "        'no.'   :   'number',\n",
    "        'gal.'  :   'gallon',\n",
    "        'tbsp.' :   'tablespoon',\n",
    "        'sq.'   :   'square',\n",
    "        'oz.'   :   'ounce',\n",
    "        'lb.'   :   'pound',\n",
    "        'qt.'   :   'quart',\n",
    "        'c.'    :   'cup',\n",
    "        'tsp.'  :   'teaspoon'\n",
    "    }\n",
    "    for item, value in __ABBREVIATIONS__.items():\n",
    "        ingredients_string = ingredients_string.lower().replace(item, value)\n",
    "    return ingredients_string\n",
    "\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    dataset.iloc[index]['ingredients'] = expand_abbreviations(dataset.iloc[index]['ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting\n",
    "Il contenuto delle colonne 'ingredients' e 'step' verrà suddiviso in frasi. In precedenza i periodi contenuti nelle singole celle sono stati formattati in modo tale da renderli riconoscibili e facilmente suddivisibili in frasi ben separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['ingredients'] = (sent_tokenize(dataset.iloc[index]['ingredients']))\n",
    "print(dataset.iloc[10]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):\n",
    "     dataset.iloc[index]['step'] = (sent_tokenize(dataset.iloc[index]['step']))\n",
    "print(dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimozione quantità doppie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_reg = r'\\d*\\s*\\(.*\\)'\n",
    "\n",
    "for index in range(len(dataset)):\n",
    "    for value in range(len(dataset.iloc[index]['ingredients'])):\n",
    "        elements = re.findall(dr_reg, dataset.iloc[index]['ingredients'][value])\n",
    "        for e in elements:\n",
    "            new_string = dataset.iloc[index]['ingredients'][value].replace(e,e[e.find(\"(\")+1: e.find(\")\")].strip())\n",
    "            dataset.iloc[index]['ingredients'][value] = new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal\n",
    "Nella colonna 'step' troviamo una serie di passaggi da compiere per creare la ricetta. Questi passaggi sono scritti in linguaggio naturale e possono essere semplificati rimuovendo delle parole dette stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in swr_dataset\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "swr_dataset = dataset.copy(deep=True)\n",
    "\n",
    "tk = lambda x,st: ' '.join([w for w in x if w not in st])\n",
    "for index in range(len(dataset)):\n",
    "    swr_dataset.iloc[index]['step'] = [tk(word_tokenize(sent), stop_words) for sent in dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming e Lemming\n",
    "Questi due processi potrebbero portare valore all'analisi del dominio. Il codice per entrambi viene proposto qui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in lem_dataset\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_dataset = swr_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(swr_dataset)):\n",
    "    lem_dataset.iloc[index]['step'] = stem_sent = [' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(sent)]) for sent in swr_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTINAL -> Risultati in stm_dataset\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stm_dataset = lem_dataset.copy(deep=True)\n",
    "\n",
    "for index in range(len(lem_dataset)):\n",
    "    stm_dataset.iloc[index]['step'] = [' '.join([stemmer.stem(w) for w in word_tokenize(sent)]) for sent in lem_dataset.iloc[index]['step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frasi originali\\n\")\n",
    "print(dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word removal\\n\")\n",
    "print(swr_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word e lemming\\n\")\n",
    "print(lem_dataset.iloc[10]['step'])\n",
    "print(\"\\nStop word lemming e stemming\\n\")\n",
    "print(stm_dataset.iloc[10]['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction and relation extraction and POS-Tagging\n",
    "Questa operazione ci permette di riconoscere le parti del testo. !!!Attualmente vengono solo stampate, ma in seguito le utilizzeremo a modo!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dataset = dataset.copy(deep=True)\n",
    "       \n",
    "for index in range(len(dataset)):\n",
    "    spacy_dataset.iloc[index]['ingredients'] = [NLP(element) for element in dataset.iloc[index]['ingredients']]\n",
    "    spacy_dataset.iloc[index]['step'] = [NLP(element) for element in dataset.iloc[index]['step']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pprintsp__(dataset, index, column, value, extend=False):\n",
    "    displacy.render(dataset.iloc[index][column][value], style='dep')\n",
    "    print(dataset.iloc[index][column][value])\n",
    "    displacy.render(dataset.iloc[index][column][value], style='ent')\n",
    "    if extend:\n",
    "        for token in dataset.iloc[index][column][value]:\n",
    "            print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "def pprint_spacyd_all(dataset, index, column, extend=False):\n",
    "    for v in range(len(dataset.iloc[index][column])):\n",
    "        __pprintsp__(dataset, index, column, v, extend)\n",
    "            \n",
    "def pprint_spacyd(dataset, index, column, value, extend=False):\n",
    "    __pprintsp__(dataset, index, column, v, extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_spacyd_all(spacy_dataset_lem, 0, 'ingredients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import X, NUM, VERB, NOUN\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_food(word):\n",
    "    syns = wn.synsets(str(word), pos = wn.NOUN)\n",
    "    for syn in syns:\n",
    "        if 'food' in syn.lexname():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#Root generation\n",
    "\n",
    "__SIZES__ = [\n",
    "       'package',\n",
    "       'tablespoon',\n",
    "       'number',\n",
    "       'pint',\n",
    "       'number',\n",
    "       'gallon',\n",
    "       'tablespoon',\n",
    "       'square',\n",
    "       'ounce',\n",
    "       'pound',\n",
    "       'quart',\n",
    "       'cup',\n",
    "       'teaspoon',\n",
    "       'can'\n",
    "]\n",
    "\n",
    "\n",
    "class Ingredient:\n",
    "    def __init__(self, full_text):\n",
    "        self.name=\"\"\n",
    "        self.size=\"\"\n",
    "        self.quantity=\"\"\n",
    "        self.size=\"\"\n",
    "        self.adj=[]\n",
    "        self.original=full_text\n",
    "    def __str__(self):\n",
    "        return f\"Name: {self.name}\\nAjdectives: {self.adj}\\nQuantity: {self.quantity} {self.size}\\nOriginal: {self.original}\"\n",
    "\n",
    "for index in range(6,10):\n",
    "    for v in range(4):\n",
    "\n",
    "        ingredient = spacy_dataset.iloc[index]['ingredients'][v]\n",
    "        pprint_spacyd(spacy_dataset, index, 'ingredients', v)\n",
    "\n",
    "        IGN = Ingredient(ingredient.text)\n",
    "        ROOT_NODE = [token for token in ingredient if token.dep_ == 'ROOT'][0]\n",
    "        \n",
    "\n",
    "        if (ROOT_NODE.pos == VERB):\n",
    "                IGN.adj.append(ROOT_NODE.text)\n",
    "        else:\n",
    "                IGN.name = ROOT_NODE.text\n",
    "                \n",
    "        stack = [element for element in ROOT_NODE.children]\n",
    "\n",
    "        while len(stack)!=0:\n",
    "            CURRENT_NODE = stack.pop()\n",
    "\n",
    "            if (CURRENT_NODE.pos == X) or (CURRENT_NODE.pos == NUM):\n",
    "                    IGN.quantity += \" \" + CURRENT_NODE.text\n",
    "            else:\n",
    "                if CURRENT_NODE.text.lower() in  __SIZES__:\n",
    "                    IGN.size += \" \" + CURRENT_NODE.text\n",
    "\n",
    "                if ((CURRENT_NODE.dep_ == 'compound') or (CURRENT_NODE.dep_ == 'dobj') or (CURRENT_NODE.dep_ == 'pobj')) and is_food(CURRENT_NODE.text) and (CURRENT_NODE.text.lower() not in  __SIZES__):\n",
    "                    IGN.name = CURRENT_NODE.text + \" \" + IGN.name\n",
    "\n",
    "                if (CURRENT_NODE.dep_ == 'amod') or (CURRENT_NODE.dep_ == 'appos'):\n",
    "                    if is_food(CURRENT_NODE.text):\n",
    "                        IGN.name = CURRENT_NODE.text + \" \" + IGN.name\n",
    "                    else:\n",
    "                        IGN.adj.append(CURRENT_NODE.text)         \n",
    "\n",
    "            stack += [element for element in CURRENT_NODE.children]   \n",
    "\n",
    "        print(IGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = set()\n",
    "for index in range(len(spacy_dataset)):\n",
    "    for doc in spacy_dataset.iloc[index]['ingredients']:\n",
    "        for token in doc:\n",
    "            string = ''\n",
    "            if token.dep_ == 'ROOT':\n",
    "                string += token.text + ' '\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == 'compound':\n",
    "                        string = child.text + ' ' + string\n",
    "                    if child.dep_ == 'amod':\n",
    "                        string = child.text + ' ' + string\n",
    "                    if child.dep_ == 'dobj':\n",
    "                        string = string + ' ' + child.text\n",
    "            ingredients.add(string)\n",
    "\n",
    "clean_ingredients = ingredients.copy()\n",
    "\n",
    "for ing in ingredients:\n",
    "    if 'teaspoon' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('teaspoon', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "    elif 'tablespoon ' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('tablespoon ', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "    elif 'cup' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('cup', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "    elif 'pound' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('pound', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "    elif 'ounce' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('ounce', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "    elif 'box' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('box', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "    elif 'package' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('package', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "    elif 'cans' in ing:\n",
    "        clean_ingredients.remove(ing)\n",
    "        ing = ing.replace('cans', ' ')\n",
    "        clean_ingredients.add(ing)\n",
    "\n",
    "print(clean_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fef19a476ade33c91668e3897e6fa668c17f7c4a539d9a83d441085d42ee0ef"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
